{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "This notebook trains as ResNet model for classifying emojis. The result is used for filtering out emojis that are difficult to recognize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "# Emoji Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from qsr_learning.entity import emoji_names, load_emoji\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, size: Tuple[int, int] = None, transform=None, max_images=0):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.transform = transform\n",
    "        self.idx2name = {}\n",
    "        self.name2idx = {}\n",
    "        for idx, name in enumerate(\n",
    "            emoji_names if not max_images else emoji_names[:max_images]\n",
    "        ):\n",
    "            self.idx2name[idx] = name\n",
    "            self.name2idx[name] = idx\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.idx2name[idx]\n",
    "        image = load_emoji(name, size=self.size)\n",
    "        # Use black background and remove the alpha channel\n",
    "        background = Image.new(\"RGBA\", image.size, (0, 0, 0))\n",
    "        image = Image.alpha_composite(background, image).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from munch import Munch\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "def report_result(epoch, phases, result, data_loader):\n",
    "    log = dict(epoch=epoch)\n",
    "    for phase in phases:\n",
    "        log[phase + \"_loss\"] = result[phase].total_loss / len(\n",
    "            data_loader[phase].dataset\n",
    "        )\n",
    "        log[phase + \"_accuracy\"] = result[phase].num_correct / len(\n",
    "            data_loader[phase].dataset\n",
    "        )\n",
    "    print(log)\n",
    "\n",
    "\n",
    "def step(model, criterion, optimizer, phase, batch, result, device):\n",
    "    images, targets = batch[0].to(device), batch[1].to(device)\n",
    "    batch_size = images.shape[0]\n",
    "    if phase == \"train\":\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        out = model(images)\n",
    "        loss = criterion(out, targets) / batch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(images)\n",
    "            loss = criterion(out, targets) / batch_size\n",
    "    result[phase].total_loss += loss.item()\n",
    "    result[phase].num_correct += (out.argmax(dim=-1) == targets).sum().item()\n",
    "\n",
    "\n",
    "def train(config, device):\n",
    "    phases = [\"train\", \"validation\"]\n",
    "    data = Munch({phase: EmojiDataset(**config.data[phase]) for phase in phases})\n",
    "    data_loader = Munch(\n",
    "        {\n",
    "            phase: DataLoader(\n",
    "                data[phase],\n",
    "                batch_size=config.train.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "            )\n",
    "            for phase in phases\n",
    "        }\n",
    "    )\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(512, len(data.train))\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    result = Munch()\n",
    "    for epoch in trange(config.train.num_epochs):\n",
    "        for phase in phases:\n",
    "            result[phase] = Munch()\n",
    "            result[phase].total_loss = 0\n",
    "            result[phase].num_correct = 0\n",
    "            for batch in data_loader[phase]:\n",
    "                step(model, criterion, optimizer, phase, batch, result, device)\n",
    "        report_result(epoch, phases, result, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Munch(\n",
    "    data=Munch(\n",
    "        train=Munch(\n",
    "            size=(224, 224),\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomAffine(\n",
    "                        degrees=(0, 360),\n",
    "                        translate=(0.345, 0.345),\n",
    "                        scale=(0.16, 0.32),\n",
    "                        shear=(-20.0, 20.0, -20.0, 20.0),\n",
    "                    ),\n",
    "                    transforms.ColorJitter(\n",
    "                        brightness=0.5, contrast=0, saturation=0.05, hue=0.05\n",
    "                    ),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.0130, 0.0114, 0.0095], std=[0.0995, 0.0885, 0.0788]),\n",
    "                ]\n",
    "            ),\n",
    "            max_images=None,\n",
    "        ),\n",
    "        validation=Munch(\n",
    "            size=(224, 224),\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomAffine(\n",
    "                        degrees=(0, 360),\n",
    "                        translate=(0.345, 0.345),\n",
    "                        scale=(0.16, 0.32),\n",
    "                        shear=(-20.0, 20.0, -20.0, 20.0),\n",
    "                    ),\n",
    "                    transforms.ColorJitter(\n",
    "                        brightness=0.5, contrast=0, saturation=0.05, hue=0.05\n",
    "                    ),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.0130, 0.0114, 0.0095], std=[0.0995, 0.0885, 0.0788]),\n",
    "                ]\n",
    "            ),\n",
    "            max_images=None,\n",
    "        ),\n",
    "    ),\n",
    "    train=Munch(batch_size=128, num_epochs=1000),\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(config, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "## Test the transformed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataloader(loader, channel_means, channel_stds):\n",
    "    #     loader = DataLoader(emoji_dataset, batch_size=1, shuffle=True)\n",
    "    for batch in loader:\n",
    "        break\n",
    "    images = batch[0]\n",
    "    image = images[0]\n",
    "    questions = batch[1]\n",
    "    question = emoji_dataset.idx2name[questions[0].item()]\n",
    "    image = Image.fromarray(\n",
    "        (\n",
    "            255\n",
    "            * (\n",
    "                image * torch.tensor(channel_stds).view(3, 1, 1)\n",
    "                + torch.tensor(channel_means).view(3, 1, 1)\n",
    "            )\n",
    "            .permute(1, 2, 0)\n",
    "            .numpy()\n",
    "        ).astype(\"uint8\")\n",
    "    )\n",
    "    display(image, question)\n",
    "\n",
    "\n",
    "channel_means = [0.0128, 0.0111, 0.0096]\n",
    "channel_stds = [0.0981, 0.0869, 0.0797]\n",
    "emoji_dataset = EmojiDataset(\n",
    "    size=(224, 224),\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomAffine(\n",
    "                degrees=(0, 360),\n",
    "                translate=(0.345, 0.345),\n",
    "                scale=(0.16, 0.32),\n",
    "                shear=(-20.0, 20.0, -20.0, 20.0),\n",
    "            ),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.5, contrast=0, saturation=0.05, hue=0.05\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=channel_means, std=channel_stds),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "loader = DataLoader(emoji_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_dataloader(\n",
    "    loader,\n",
    "    channel_means=channel_means,\n",
    "    channel_stds=channel_stds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "## Compute the mean and the std of the emoji images for each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dataset = EmojiDataset(\n",
    "    size=(224, 224),\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomAffine(\n",
    "                degrees=(0, 360),\n",
    "                translate=(0.345, 0.345),\n",
    "                scale=(0.16, 0.32),\n",
    "                shear=(-20.0, 20.0, -20.0, 20.0),\n",
    "            ),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.5, contrast=0, saturation=0.05, hue=0.05\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "loader = DataLoader(emoji_dataset, batch_size=1, shuffle=True)\n",
    "channel_values = (\n",
    "    torch.stack([img[0] for img, _ in loader], dim=0).permute(1, 0, 2, 3).reshape(3, -1)\n",
    ")\n",
    "print(\"mean:\", channel_values.mean(dim=1))\n",
    "print(\"std:\", channel_values.view(3, -1).std(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "- mean = [0.1948, 0.2264, 0.1711]\n",
    "- std = [0.3252, 0.3362, 0.3034]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
