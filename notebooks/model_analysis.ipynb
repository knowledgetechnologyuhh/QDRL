{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from munch import Munch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from qsr_learning.data import DRLDataset\n",
    "from qsr_learning.models import DRLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Munch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac_language_encoder_kwargs = {\n",
    "    'encoder_type': 'lstm',\n",
    "    'decoder_type': 'linear',\n",
    "    'null_token': None,\n",
    "    'encoder_vocab_size': None,\n",
    "    'wordvec_dim': 300,\n",
    "    'hidden_dim': 128,\n",
    "    'rnn_num_layers': 1,\n",
    "    'rnn_dropout': 0,\n",
    "    'parameter_efficient': True,\n",
    "    'output_batchnorm': False,\n",
    "    'bidirectional': True,\n",
    "    'gamma_option': 'linear', # not used\n",
    "    'gamma_baseline': 1, # not used\n",
    "    'use_attention': False, # not used\n",
    "    'taking_context': True,\n",
    "    'variational_embedding_dropout': 0.,\n",
    "    'embedding_uniform_boundary': 1.0,\n",
    "    'module_num_layers': 1,\n",
    "    'num_modules': None, # from the mac model kwargs\n",
    "}\n",
    "\n",
    "film_language_encoder_kwargs = {\n",
    "    'encoder_type': 'gru',\n",
    "    'decoder_type': 'linear',\n",
    "    'null_token': None,\n",
    "    'encoder_vocab_size': None,\n",
    "    'wordvec_dim': 200,\n",
    "    'hidden_dim': 1024,\n",
    "    'rnn_num_layers': 1,\n",
    "    'rnn_dropout': 0,\n",
    "    'parameter_efficient': True,\n",
    "    'output_batchnorm': False,\n",
    "    'bidirectional': False,\n",
    "    'gamma_option': 'linear', # not used\n",
    "    'gamma_baseline': 1, # not used\n",
    "    'use_attention': False, # not used\n",
    "    \"num_modules\": None,\n",
    "}\n",
    "\n",
    "mac_kwargs = {\n",
    "    'vocab': None,\n",
    "    'feature_dim': None, # raw images: [3,128,128]; resnet18 features: [256, 14, 14]\n",
    "    'stem_num_layers': 6,\n",
    "    'stem_batchnorm': True,\n",
    "    'stem_kernel_size': [3],\n",
    "    'stem_subsample_layers': [1,3], # add MaxPool2d(kernel_size=2, stride=2)\n",
    "    'stem_stride': [1],\n",
    "    'stem_padding': None,\n",
    "    'stem_dim': 64,\n",
    "    'num_modules': None,\n",
    "    'module_dim': 128,\n",
    "    'question_embedding_dropout': 0.,\n",
    "    'stem_dropout': 0.,\n",
    "    'memory_dropout': 0.,\n",
    "    'read_dropout': 0.,\n",
    "    'nonlinearity': 'ELU',\n",
    "    'use_prior_control_in_control_unit': 0 == 1,\n",
    "    'use_self_attention': 0,\n",
    "    'use_memory_gate': 0,\n",
    "    'question2output': 1,\n",
    "    'classifier_batchnorm': 0 == 1,\n",
    "    'classifier_fc_layers': [1024],\n",
    "    'classifier_dropout': 0.,\n",
    "    'use_coords': 1, # 1, 0\n",
    "    'write_unit': 'original',\n",
    "    'read_connect': 'last',\n",
    "    'noisy_controls': bool(0),\n",
    "    'debug_every': float('inf'),\n",
    "    'print_verbose_every': float('inf'),\n",
    "    'hard_code_control' : False\n",
    "    }\n",
    "    \n",
    "film_kwargs = {\n",
    "    'vocab': None,\n",
    "    'feature_dim': None, # raw images: [3,128,128]; resnet18 features: [256, 14, 14]\n",
    "    'stem_num_layers': 6,\n",
    "    'stem_batchnorm': True,\n",
    "    'stem_kernel_size': [3],\n",
    "    'stem_subsample_layers': [1,3], # add MaxPool2d(kernel_size=2, stride=2)\n",
    "    'stem_stride': [1],\n",
    "    'stem_padding': None,\n",
    "    'stem_dim': 64,\n",
    "    'num_modules': None,\n",
    "    'module_num_layers': 1,\n",
    "    'module_dim': 64, #was 128 in original FiLM`\n",
    "    'module_residual': True,\n",
    "    'module_intermediate_batchnorm': False,\n",
    "    'module_batchnorm': True,\n",
    "    'module_batchnorm_affine': False,\n",
    "    'module_dropout': 0e-2,\n",
    "    'module_input_proj': 1,\n",
    "    'module_kernel_size': 3,\n",
    "    'classifier_proj_dim': 512,\n",
    "    'classifier_downsample': 'maxpool2',\n",
    "    'classifier_fc_layers': [1024],\n",
    "    'classifier_batchnorm': True,\n",
    "    'classifier_dropout': 0,\n",
    "    'condition_method': 'bn-film',\n",
    "    'condition_pattern': None, # [1,1,1,1], # length should be equal to \"num_modules\"\n",
    "    'use_gamma': True,\n",
    "    'use_beta': True,\n",
    "    'use_coords': 1, # 1, 0\n",
    "    'debug_every': float('inf'),\n",
    "    'print_verbose_every': float('inf'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def load_checkpoint(checkpoint):\n",
    "    if os.path.isfile(checkpoint):\n",
    "        print(\"=> loading checkpoint '{}'\".format(checkpoint))\n",
    "        cp = torch.load(checkpoint)\n",
    "        return cp\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(checkpoint))\n",
    "        raise Exception(\"No checkpoint found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2 entities, intrinsic\n",
    "# root_datadir = '/data/mli/qsr-learning/qsr-learning/data/intrinsic/torch_random_seed_1/num_entity_2/excluded_entity_18_excluded_relation_all4/image_size_128_emoji_size_24/'\n",
    "# checkpoint_dirname = '2021-03-28 11:15:14.474585+02:00_film_num_modules_4' \n",
    "# checkpoint_dirname = '2021-03-28 23:12:55.798907+02:00_mac_num_modules_2'\n",
    "\n",
    "#### 5 entities, intrinsic\n",
    "root_datadir = '/data/mli/qsr-learning/qsr-learning/data/intrinsic/torch_random_seed_1/num_entity_5/excluded_entity_18_excluded_relation_all4/image_size_128_emoji_size_24/'\n",
    "checkpoint_dirname = '2021-03-28 23:30:41.039363+02:00_mac_num_modules_2'\n",
    "checkpoint_filename = 'final'\n",
    "checkpoint_path = os.path.join(root_datadir, checkpoint_dirname, 'Checkpoints', 'ckpt_{}.pth.tar'.format(checkpoint_filename))\n",
    "model_type = 'mac'\n",
    "number_modules = 2\n",
    "num_entity = 5\n",
    "reference_type = 'intrinsic'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "cp = load_checkpoint(checkpoint_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_encoder = None\n",
    "model = None\n",
    "vocab = None\n",
    "vocab = cp['vocab']\n",
    "len(vocab['question_token_to_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vr.models import MAC, FiLMedNet, FiLMGen\n",
    "if model_type == \"mac\":\n",
    "    mac_language_encoder_kwargs['null_token'] = vocab['question_token_to_idx']['NULL']\n",
    "    mac_language_encoder_kwargs['encoder_vocab_size'] = len(vocab['question_token_to_idx'])\n",
    "    mac_language_encoder_kwargs['num_modules'] = number_modules\n",
    "    mac_kwargs['vocab'] = vocab\n",
    "    mac_kwargs['feature_dim'] = [3,128,128]\n",
    "    mac_kwargs['num_modules'] = number_modules\n",
    "    language_encoder = FiLMGen(**mac_language_encoder_kwargs).to(device)\n",
    "    model = MAC(**mac_kwargs).to(device)\n",
    "elif model_type == \"film\":\n",
    "    film_language_encoder_kwargs['null_token'] = vocab['question_token_to_idx']['NULL']\n",
    "    film_language_encoder_kwargs['encoder_vocab_size'] = len(vocab['question_token_to_idx'])\n",
    "    film_language_encoder_kwargs['num_modules'] = number_modules\n",
    "    film_kwargs['vocab'] = vocab\n",
    "    film_kwargs['feature_dim'] = [3,128,128]\n",
    "    film_kwargs['num_modules'] = number_modules\n",
    "    film_kwargs['condition_pattern'] = [1] * number_modules\n",
    "    language_encoder = FiLMGen(**film_language_encoder_kwargs).to(device)\n",
    "    model = FiLMedNet(**film_kwargs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_encoder.load_state_dict(cp[\"language_encoder_state\"])\n",
    "model.load_state_dict(cp[\"model_state\"])\n",
    "language_encoder.eval()\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qsr_learning.entity import emoji_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_names = [\n",
    "    \"slightly smiling face\",\n",
    "    \"nerd face\",\n",
    "    \"smiling face with halo\",\n",
    "    \"expressionless face\",\n",
    "    \"flushed face\",\n",
    "    \"face with tears of joy\",\n",
    "    \"neutral face\",\n",
    "    \"smiling face with heart-eyes\",\n",
    "    \"face with medical mask\",\n",
    "    \"loudly crying face\",\n",
    "    \"hugging face\",\n",
    "    \"smiling face with smiling eyes\",\n",
    "    \"squinting face with tongue\",\n",
    "    \"face with steam from nose\",\n",
    "    \"dog face\",\n",
    "    \"cat face\",\n",
    "    \"face screaming in fear\",\n",
    "    \"pouting face\",\n",
    "    \"pig face\",\n",
    "    \"rabbit face\",\n",
    "    \"tiger face\",\n",
    "    \"monkey face\",\n",
    "    \"cow face\",\n",
    "    \"tired face\",\n",
    "    \"mouse face\",\n",
    "    \"dragon face\",\n",
    "    \"face with tongue\",\n",
    "    \"sun with face\",\n",
    "    \"worried face\",\n",
    "    \"dizzy face\",\n",
    "    \"face with open mouth\",\n",
    "    \"fearful face\",\n",
    "]\n",
    "\n",
    "excluded_entity_names = [\n",
    "    \"hugging face\",\n",
    "    \"fearful face\",\n",
    "    \"face with steam from nose\",\n",
    "    \"face with tongue\",\n",
    "    \"nerd face\",\n",
    "    \"expressionless face\",\n",
    "    \"dragon face\",\n",
    "    \"flushed face\",\n",
    "    \"cow face\",\n",
    "    \"smiling face with heart-eyes\",\n",
    "    \"sun with face\",\n",
    "    \"pig face\",\n",
    "    \"pouting face\",\n",
    "    \"smiling face with halo\",\n",
    "    \"slightly smiling face\",\n",
    "    \"worried face\",\n",
    "    \"neutral face\",\n",
    "    \"loudly crying face\",\n",
    "]\n",
    "\n",
    "relation_names_abs_intri = [\"left_of\", \"right_of\", \"above\", \"below\"]\n",
    "excluded_relation_names_all4_abs_intri = [\"left_of\", \"right_of\", \"above\", \"below\"]\n",
    "\n",
    "\n",
    "relation_names_rel = [\"left_of\", \"right_of\", \"in_front_of\", \"behind\"]\n",
    "excluded_relation_names_all4_rel = [\"left_of\", \"right_of\", \"in_front_of\", \"behind\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reference_type == \"intrinsic\" or args.reference_type == \"absolute\":\n",
    "        relation_names = relation_names_abs_intri\n",
    "        excluded_relation_names = excluded_relation_names_all4_abs_intri\n",
    "        pass\n",
    "elif reference_type == \"relative\":\n",
    "    relation_names = relation_names_rel\n",
    "    excluded_relation_names = excluded_relation_names_all4_rel\n",
    "    pass\n",
    "else:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.dataset = Munch(\n",
    "        vocab=list(vocab[\"question_token_to_idx\"].keys()),\n",
    "        entity_names=entity_names,\n",
    "        relation_names=relation_names,\n",
    "        num_entities=num_entity,\n",
    "        frame_of_reference= \"intrinsic\", # \"intrinsic\" or \"absolute\"\n",
    "        w_range=(24, 24), #(8, 8) (16, 16) (24, 24) (32, 32)\n",
    "        h_range=(24, 24), #(8, 8)\n",
    "        theta_range=(0, 2 * math.pi),\n",
    "        add_bbox=False,\n",
    "        add_front=False,\n",
    "        transform=None,\n",
    "        canvas_size=(128, 128), #(224, 224) (128, 128) (64, 64)\n",
    "        num_samples=1000,\n",
    "        root_seed=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config.data_loader = Munch(\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "test_dataset = DRLDataset(\n",
    "            **{\n",
    "                **config.dataset,\n",
    "                **dict(\n",
    "                    entity_names=excluded_entity_names,\n",
    "                    excluded_entity_names=[],\n",
    "                    relation_names=excluded_relation_names,\n",
    "                    excluded_relation_names=[],\n",
    "                    num_samples=100,\n",
    "                    root_seed=10 ** 7,\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "test_loader = DataLoader(\n",
    "        test_dataset, **{**config.data_loader, \"shuffle\": False}\n",
    "    )\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 entities\n",
    "from ipywidgets import interact\n",
    "from qsr_learning.binary_relation import above, below, left_of, right_of\n",
    "from qsr_learning.data import DRLDataset\n",
    "\n",
    "import math\n",
    "from PIL import Image\n",
    "from qsr_learning.entity import Entity\n",
    "\n",
    "dataset = test_dataset\n",
    "\n",
    "question = [\"face with open mouth\", \"below\", \"hugging face\"]\n",
    "question_t = torch.tensor([dataset.word2idx[w] for w in question], dtype=torch.long)\n",
    "# answer = 0\n",
    "# answer_t = torch.tensor(answer, dtype=torch.float)\n",
    "\n",
    "@interact(\n",
    "    frame_of_reference=(0, 1),\n",
    "    x1=(0, 105),\n",
    "    y1=(0, 105),\n",
    "    theta1=(0, 360),\n",
    "    x2=(0, 105),\n",
    "    y2=(0, 105),\n",
    "    theta2=(0, 360),\n",
    ")\n",
    "def test_spatial_relations(\n",
    "    frame_of_reference=0, x1=30, y1=30, theta1=0, x2=64, y2=64, theta2=150\n",
    "):\n",
    "    canvas = Image.new(\"RGBA\", (128, 128), (0, 0, 0, 255))\n",
    "    entity1 = Entity(\n",
    "        name=\"face with open mouth\",\n",
    "        frame_of_reference={0: \"absolute\", 1: \"intrinsic\"}[frame_of_reference],\n",
    "        p=(x1, y1),\n",
    "        theta=theta1 / 360 * 2 * math.pi,\n",
    "        size=(24, 24),\n",
    "    )\n",
    "    entity1.draw(canvas, add_bbox=False, add_front=False)\n",
    "    entity2 = Entity(\n",
    "        name=\"hugging face\",\n",
    "        frame_of_reference={0: \"absolute\", 1: \"intrinsic\"}[frame_of_reference],\n",
    "        p=(x2, y2),\n",
    "        theta=theta2 / 360 * 2 * math.pi,\n",
    "        size=(24, 24),\n",
    "    )\n",
    "    entity2.draw(canvas, add_bbox=False, add_front=False)\n",
    "    background = Image.new(\"RGBA\", canvas.size, (0, 0, 0))\n",
    "    image = Image.alpha_composite(background, canvas).convert(\"RGB\")\n",
    "    display(image)\n",
    "    \n",
    "    image_t = dataset.transform(image)\n",
    "    questions_var = question_t.unsqueeze(0).to(device)\n",
    "    questions_feat = language_encoder(questions_var)\n",
    "    images_var = image_t.unsqueeze(0).to(device)\n",
    "    if model_type == \"mac\":\n",
    "        scores = model(images_var, questions_feat, isTest=True)\n",
    "    elif model_type == \"film\":\n",
    "        scores = model(images_var, questions_feat)\n",
    "        \n",
    "    if scores is not None:\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        print(\"Question:\", ' '.join(question))\n",
    "        print(\"Prediction: \", bool(preds.item()))\n",
    "        print(\"scores:\", scores.data.softmax(dim=1)[0,1].item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5 entities\n",
    "from ipywidgets import interact\n",
    "from qsr_learning.binary_relation import above, below, left_of, right_of\n",
    "from qsr_learning.data import DRLDataset\n",
    "\n",
    "import math\n",
    "from PIL import Image\n",
    "from qsr_learning.entity import Entity\n",
    "\n",
    "dataset = test_dataset\n",
    "\n",
    "question = [\"face with tongue\", \"above\", \"nerd face\"]\n",
    "question_t = torch.tensor([dataset.word2idx[w] for w in question], dtype=torch.long)\n",
    "# answer = 0\n",
    "# answer_t = torch.tensor(answer, dtype=torch.float)\n",
    "\n",
    "@interact(\n",
    "    frame_of_reference=(0, 1),\n",
    "    x1=(0, 105),\n",
    "    y1=(0, 105),\n",
    "    theta1=(0, 360),\n",
    "    x2=(0, 105),\n",
    "    y2=(0, 105),\n",
    "    theta2=(0, 360),\n",
    "    x3=(0, 105),\n",
    "    y3=(0, 105),\n",
    "    theta3=(0, 360),\n",
    "    x4=(0, 105),\n",
    "    y4=(0, 105),\n",
    "    theta4=(0, 360),\n",
    "    x5=(0, 105),\n",
    "    y5=(0, 105),\n",
    "    theta5=(0, 360),\n",
    ")\n",
    "def test_spatial_relations(\n",
    "    frame_of_reference=0, x1=30, y1=30, theta1=0, x2=64, y2=64, theta2=150, \\\n",
    "    x3=10, y3=10, theta3=0, x4=70, y4=10, theta4=150, x5=10, y5=60, theta5=0\n",
    "):\n",
    "    canvas = Image.new(\"RGBA\", (128, 128), (0, 0, 0, 255))\n",
    "    entity1 = Entity(\n",
    "        name=\"nerd face\",\n",
    "        frame_of_reference={0: \"absolute\", 1: \"intrinsic\"}[frame_of_reference],\n",
    "        p=(x1, y1),\n",
    "        theta=theta1 / 360 * 2 * math.pi,\n",
    "        size=(24, 24),\n",
    "    )\n",
    "    entity1.draw(canvas, add_bbox=False, add_front=False)\n",
    "    entity2 = Entity(\n",
    "        name=\"face with tongue\",\n",
    "        frame_of_reference={0: \"absolute\", 1: \"intrinsic\"}[frame_of_reference],\n",
    "        p=(x2, y2),\n",
    "        theta=theta2 / 360 * 2 * math.pi,\n",
    "        size=(24, 24),\n",
    "    )\n",
    "    entity2.draw(canvas, add_bbox=False, add_front=False)\n",
    "    \n",
    "    entity3 = Entity(\n",
    "        name=\"slightly smiling face\",\n",
    "        frame_of_reference={0: \"absolute\", 1: \"intrinsic\"}[frame_of_reference],\n",
    "        p=(x3, y3),\n",
    "        theta=theta3 / 360 * 2 * math.pi,\n",
    "        size=(24, 24),\n",
    "    )\n",
    "    entity3.draw(canvas, add_bbox=False, add_front=False)\n",
    "    \n",
    "    entity4 = Entity(\n",
    "        name=\"cow face\",\n",
    "        frame_of_reference={0: \"absolute\", 1: \"intrinsic\"}[frame_of_reference],\n",
    "        p=(x4, y4),\n",
    "        theta=theta4 / 360 * 2 * math.pi,\n",
    "        size=(24, 24),\n",
    "    )\n",
    "    entity4.draw(canvas, add_bbox=False, add_front=False)\n",
    "    \n",
    "    entity5 = Entity(\n",
    "        name=\"pig face\",\n",
    "        frame_of_reference={0: \"absolute\", 1: \"intrinsic\"}[frame_of_reference],\n",
    "        p=(x5, y5),\n",
    "        theta=theta5 / 360 * 2 * math.pi,\n",
    "        size=(24, 24),\n",
    "    )\n",
    "    entity5.draw(canvas, add_bbox=False, add_front=False)\n",
    "    \n",
    "    background = Image.new(\"RGBA\", canvas.size, (0, 0, 0))\n",
    "    image = Image.alpha_composite(background, canvas).convert(\"RGB\")\n",
    "    display(image)\n",
    "    \n",
    "    image_t = dataset.transform(image)\n",
    "    questions_var = question_t.unsqueeze(0).to(device)\n",
    "    questions_feat = language_encoder(questions_var)\n",
    "    images_var = image_t.unsqueeze(0).to(device)\n",
    "    if model_type == \"mac\":\n",
    "        scores = model(images_var, questions_feat, isTest=True)\n",
    "    elif model_type == \"film\":\n",
    "        scores = model(images_var, questions_feat)\n",
    "        \n",
    "    if scores is not None:\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        print(\"Question:\", ' '.join(question))\n",
    "        print(\"Prediction: \", bool(preds.item()))\n",
    "        print(\"scores:\", scores.data.softmax(dim=1)[0,1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "cnt = 0\n",
    "dataset = test_dataset\n",
    "for idx in range(len(dataset)):\n",
    "    image_t, question_t, answer_t = dataset[idx]\n",
    "    questions_var = question_t.unsqueeze(0).to(device)\n",
    "    questions_feat = language_encoder(questions_var)\n",
    "    images_var = image_t.unsqueeze(0).to(device)\n",
    "    answers_var = answer_t.unsqueeze(0).type(torch.LongTensor).to(device)\n",
    "    if model_type == \"mac\":\n",
    "        scores = model(images_var, questions_feat, isTest=True)\n",
    "    elif model_type == \"film\":\n",
    "        scores = model(images_var, questions_feat)\n",
    "\n",
    "    if scores is not None:\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        if preds == answer_t:\n",
    "            pass\n",
    "            # print(\"correct!\")\n",
    "        else:\n",
    "            cnt += 1\n",
    "            image = Image.fromarray(\n",
    "                (255 * (dataset.std.view(-1, 1, 1) * image_t + dataset.mean.view(-1, 1, 1)))\n",
    "                .permute(1, 2, 0)\n",
    "                .numpy()\n",
    "                .astype(\"uint8\")\n",
    "            )\n",
    "            question = [dataset.idx2word[idx] for idx in question_t.tolist()]\n",
    "            display(image)\n",
    "            print(question)\n",
    "            print(\"Ground truth: \", bool(answer_t))\n",
    "            print(\"Prediction: \", bool(preds.item()))\n",
    "            print(\"scores:\", scores.data.softmax(dim=1)[0,1].item())\n",
    "            print(\"wroung!\")\n",
    "print(\"cnt:{}\".format(cnt))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from PIL import Image\n",
    "dataset = test_dataset\n",
    "@interact(idx=(0, len(dataset) - 1))\n",
    "def display_sample(idx=0):\n",
    "\n",
    "    image_t, question_t, answer_t = dataset[idx]\n",
    "    image = Image.fromarray(\n",
    "        (255 * (dataset.std.view(-1, 1, 1) * image_t + dataset.mean.view(-1, 1, 1)))\n",
    "        .permute(1, 2, 0)\n",
    "        .numpy()\n",
    "        .astype(\"uint8\")\n",
    "    )\n",
    "\n",
    "    question = [dataset.idx2word[idx] for idx in question_t.tolist()]\n",
    "    answer = bool(answer_t)\n",
    "    display(image)\n",
    "    print(question)\n",
    "    print(\"Ground truth: \", answer)\n",
    "    \n",
    "    #(images, questions, answers) = batch # images.shape: [32, 3, 224, 224]\n",
    "    questions_var = question_t.unsqueeze(0).to(device)\n",
    "    questions_feat = language_encoder(questions_var)\n",
    "    images_var = image_t.unsqueeze(0).to(device)\n",
    "    answers_var = answer_t.unsqueeze(0).type(torch.LongTensor).to(device)\n",
    "    if model_type == \"mac\":\n",
    "        scores = model(images_var, questions_feat, isTest=True)\n",
    "    elif model_type == \"film\":\n",
    "        scores = model(images_var, questions_feat)\n",
    "\n",
    "    if scores is not None:\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        print(\"Prediction: \", bool(preds.item()))\n",
    "        if preds == answer_t:\n",
    "            print(\"correct!\")\n",
    "        else:\n",
    "            print(\"wroung!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.data_loader = Munch(\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, **config.data_loader)\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset, **{**config.data_loader, \"shuffle\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model = Munch(\n",
    "    vision_model=\"resnet18\",\n",
    "    image_size=(3, *config.dataset.canvas_size),\n",
    "    num_embeddings=len(dataset.word2idx),\n",
    "    embedding_dim=10,\n",
    "    question_len=dataset[0][1].shape.numel(),\n",
    ")\n",
    "\n",
    "model = DRLNet(**config.model)\n",
    "lightning_checkpoint_path = (\n",
    "    \"lightning_logs/version_29/checkpoints/epoch=99-step=19599.ckpt\"\n",
    ")\n",
    "model.load_state_dict(torch.load(lightning_checkpoint_path)[\"state_dict\"])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on manual datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from ipywidgets import interact\n",
    "from PIL import Image\n",
    "\n",
    "from qsr_learning.data import Question, draw_entities\n",
    "from qsr_learning.entity import Entity\n",
    "from qsr_learning.relation import above, below, left_of, right_of\n",
    "\n",
    "\n",
    "@interact(\n",
    "    frame_of_reference=(0, 1),\n",
    "    x1=(0, 190),\n",
    "    y1=(0, 190),\n",
    "    theta1=(0, 360),\n",
    "    x2=(0, 190),\n",
    "    y2=(0, 190),\n",
    "    theta2=(0, 360),\n",
    ")\n",
    "def test_spatial_relations(\n",
    "    frame_of_reference=1, x1=64, y1=64, theta1=0, x2=128, y2=128, theta2=150\n",
    "):\n",
    "    canvas = Image.new(\"RGBA\", (224, 224), (127, 127, 127, 127))\n",
    "    entity1 = Entity(\n",
    "        name=\"octopus\",\n",
    "        frame_of_reference={0: \"absolute\", 1: \"intrinsic\"}[frame_of_reference],\n",
    "        p=(x1, y1),\n",
    "        theta=theta1 / 360 * 2 * math.pi,\n",
    "        size=(32, 32),\n",
    "    )\n",
    "    entity2 = Entity(\n",
    "        name=\"trophy\",\n",
    "        frame_of_reference={0: \"absolute\", 1: \"intrinsic\"}[frame_of_reference],\n",
    "        p=(x2, y2),\n",
    "        theta=theta2 / 360 * 2 * math.pi,\n",
    "        size=(32, 32),\n",
    "    )\n",
    "    image = draw_entities([entity1, entity2], add_bbox=True)\n",
    "    background = Image.new(\"RGBA\", image.size, (0, 0, 0))\n",
    "    image = Image.alpha_composite(background, image).convert(\"RGB\")\n",
    "    display(image)\n",
    "    image_t = dataset.transform(image)\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for relation in [right_of]:\n",
    "        questions.append(Question(entity1.name, relation.__name__, entity2.name))\n",
    "        answers.append(relation(entity1, entity2))\n",
    "    #     for relation in dataset.relations:\n",
    "    #         questions.append(Question(entity2.name, relation.__name__, entity1.name))\n",
    "    #         answers.append(relation(entity2, entity1))\n",
    "    for question, answer in zip(questions, answers):\n",
    "        question_t = torch.tensor([dataset.word2idx[word] for word in question])\n",
    "        answer_t = torch.tensor(answer)\n",
    "        with torch.no_grad():\n",
    "            pred_t = model(image_t.unsqueeze(0), question_t.unsqueeze(0))\n",
    "        score = pred_t.sigmoid().item()\n",
    "        pred = bool(pred_t.sigmoid().round())\n",
    "        print(\n",
    "            f\"\\n{question.head:7} {question.relation:8} {question.tail:7}\\n\\nGround Truth: {answer}\\nPrediction  : {pred}\\nScore       : {score:3.2f}\\nCorrect     : {answer==pred:1}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display incorrect predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Getteng samples predicted incorrectly does not work yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "idx_incorrect = []\n",
    "model.to(device)\n",
    "with tqdm(total=(len(validation_dataset) // config.data_loader.batch_size + 1)) as pbar:\n",
    "    for (i, batch) in enumerate(validation_loader):\n",
    "        batch_size = batch[0].shape[0]\n",
    "        image = batch[0].to(device)\n",
    "        question = batch[1].to(device)\n",
    "        answer = batch[2].to(device)\n",
    "        idx = torch.arange(i * batch_size, (i + 1) * batch_size)\n",
    "        idx_incorrect.extend(\n",
    "            idx[answer != model(image, question).sigmoid().round()].tolist()\n",
    "        )\n",
    "        pbar.update(1)\n",
    "model.to(torch.device(\"cpu\"))\n",
    "\n",
    "from ipywidgets import interact\n",
    "from PIL import Image\n",
    "\n",
    "subset = validation_dataset\n",
    "\n",
    "\n",
    "@interact(idx=(0, len(idx_incorrect) - 1))\n",
    "def display_sample(idx=0):\n",
    "    idx = idx_incorrect[idx]\n",
    "    image_t, question_t, answer_t = subset[idx]\n",
    "    with torch.no_grad():\n",
    "        pred_t = model(image_t.unsqueeze(0), question_t.unsqueeze(0))\n",
    "    image = Image.fromarray(\n",
    "        (255 * (dataset.std.view(-1, 1, 1) * image_t + dataset.mean.view(-1, 1, 1)))\n",
    "        .permute(1, 2, 0)\n",
    "        .numpy()\n",
    "        .astype(\"uint8\")\n",
    "    )\n",
    "    head, relation, tail = question_t.tolist()\n",
    "    question = (\n",
    "        dataset.idx2word[head],\n",
    "        dataset.idx2word[relation],\n",
    "        dataset.idx2word[tail],\n",
    "    )\n",
    "    answer = bool(answer_t)\n",
    "    pred = bool(pred_t.round())\n",
    "    display(image)\n",
    "    print(question)\n",
    "    print(\"Ground truth: \", answer)\n",
    "    print(\"Prediction: \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
