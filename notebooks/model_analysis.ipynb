{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from munch import Munch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from qsr_learning.data import DRLDataset\n",
    "from qsr_learning.models import DRLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Munch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qsr_learning.entity import emoji_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.dataset = Munch(\n",
    "    entity_names=[\"octopus\", \"trophy\"],\n",
    "    relation_names=[\"left_of\", \"right_of\"],\n",
    "    num_entities=2,\n",
    "    frame_of_reference=\"intrinsic\",\n",
    "    w_range=(32, 32),\n",
    "    h_range=(32, 32),\n",
    "    theta_range=(0, 2 * math.pi),\n",
    "    add_bbox=False,\n",
    "    add_front=False,\n",
    "    transform=None,\n",
    "    canvas_size=(224, 224),\n",
    "    num_samples=10 ** 5 + 10 ** 4 + 10 ** 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DRLDataset(**config.dataset)\n",
    "train_dataset, validation_dataset, test_dataset = random_split(\n",
    "    dataset,\n",
    "    [10 ** 5, 10 ** 4, 10 ** 4],\n",
    "    generator=torch.Generator().manual_seed(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.data_loader = Munch(\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, **config.data_loader)\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset, **{**config.data_loader, \"shuffle\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model = Munch(\n",
    "    vision_model=\"resnet18\",\n",
    "    image_size=(3, *config.dataset.canvas_size),\n",
    "    num_embeddings=len(dataset.word2idx),\n",
    "    embedding_dim=10,\n",
    "    question_len=dataset[0][1].shape.numel(),\n",
    ")\n",
    "\n",
    "model = DRLNet(**config.model)\n",
    "lightning_checkpoint_path = (\n",
    "    \"lightning_logs/version_29/checkpoints/epoch=99-step=19599.ckpt\"\n",
    ")\n",
    "model.load_state_dict(torch.load(lightning_checkpoint_path)[\"state_dict\"])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "## Evaluate on manual datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from ipywidgets import interact\n",
    "from PIL import Image\n",
    "\n",
    "from qsr_learning.data import Question, draw_entities\n",
    "from qsr_learning.entity import Entity\n",
    "from qsr_learning.relation import above, below, left_of, right_of\n",
    "\n",
    "\n",
    "@interact(\n",
    "    frame_of_reference=(0, 1),\n",
    "    x1=(0, 190),\n",
    "    y1=(0, 190),\n",
    "    theta1=(0, 360),\n",
    "    x2=(0, 190),\n",
    "    y2=(0, 190),\n",
    "    theta2=(0, 360),\n",
    ")\n",
    "def test_spatial_relations(\n",
    "    frame_of_reference=1, x1=64, y1=64, theta1=0, x2=128, y2=128, theta2=150\n",
    "):\n",
    "    canvas = Image.new(\"RGBA\", (224, 224), (127, 127, 127, 127))\n",
    "    entity1 = Entity(\n",
    "        name=\"octopus\",\n",
    "        frame_of_reference={0: \"absolute\", 1: \"intrinsic\"}[frame_of_reference],\n",
    "        p=(x1, y1),\n",
    "        theta=theta1 / 360 * 2 * math.pi,\n",
    "        size=(32, 32),\n",
    "    )\n",
    "    entity2 = Entity(\n",
    "        name=\"trophy\",\n",
    "        frame_of_reference={0: \"absolute\", 1: \"intrinsic\"}[frame_of_reference],\n",
    "        p=(x2, y2),\n",
    "        theta=theta2 / 360 * 2 * math.pi,\n",
    "        size=(32, 32),\n",
    "    )\n",
    "    image = draw_entities([entity1, entity2], add_bbox=True)\n",
    "    background = Image.new(\"RGBA\", image.size, (0, 0, 0))\n",
    "    image = Image.alpha_composite(background, image).convert(\"RGB\")\n",
    "    display(image)\n",
    "    image_t = dataset.transform(image)\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for relation in [right_of]:\n",
    "        questions.append(Question(entity1.name, relation.__name__, entity2.name))\n",
    "        answers.append(relation(entity1, entity2))\n",
    "    #     for relation in dataset.relations:\n",
    "    #         questions.append(Question(entity2.name, relation.__name__, entity1.name))\n",
    "    #         answers.append(relation(entity2, entity1))\n",
    "    for question, answer in zip(questions, answers):\n",
    "        question_t = torch.tensor([dataset.word2idx[word] for word in question])\n",
    "        answer_t = torch.tensor(answer)\n",
    "        with torch.no_grad():\n",
    "            pred_t = model(image_t.unsqueeze(0), question_t.unsqueeze(0))\n",
    "        score = pred_t.sigmoid().item()\n",
    "        pred = bool(pred_t.sigmoid().round())\n",
    "        print(\n",
    "            f\"\\n{question.head:7} {question.relation:8} {question.tail:7}\\n\\nGround Truth: {answer}\\nPrediction  : {pred}\\nScore       : {score:3.2f}\\nCorrect     : {answer==pred:1}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "## Display incorrect predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "TODO: Getteng samples predicted incorrectly does not work yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "idx_incorrect = []\n",
    "model.to(device)\n",
    "with tqdm(total=(len(validation_dataset) // config.data_loader.batch_size + 1)) as pbar:\n",
    "    for (i, batch) in enumerate(validation_loader):\n",
    "        batch_size = batch[0].shape[0]\n",
    "        image = batch[0].to(device)\n",
    "        question = batch[1].to(device)\n",
    "        answer = batch[2].to(device)\n",
    "        idx = torch.arange(i * batch_size, (i + 1) * batch_size)\n",
    "        idx_incorrect.extend(\n",
    "            idx[answer != model(image, question).sigmoid().round()].tolist()\n",
    "        )\n",
    "        pbar.update(1)\n",
    "model.to(torch.device(\"cpu\"))\n",
    "\n",
    "from ipywidgets import interact\n",
    "from PIL import Image\n",
    "\n",
    "subset = validation_dataset\n",
    "\n",
    "\n",
    "@interact(idx=(0, len(idx_incorrect) - 1))\n",
    "def display_sample(idx=0):\n",
    "    idx = idx_incorrect[idx]\n",
    "    image_t, question_t, answer_t = subset[idx]\n",
    "    with torch.no_grad():\n",
    "        pred_t = model(image_t.unsqueeze(0), question_t.unsqueeze(0))\n",
    "    image = Image.fromarray(\n",
    "        (255 * (dataset.std.view(-1, 1, 1) * image_t + dataset.mean.view(-1, 1, 1)))\n",
    "        .permute(1, 2, 0)\n",
    "        .numpy()\n",
    "        .astype(\"uint8\")\n",
    "    )\n",
    "    head, relation, tail = question_t.tolist()\n",
    "    question = (\n",
    "        dataset.idx2word[head],\n",
    "        dataset.idx2word[relation],\n",
    "        dataset.idx2word[tail],\n",
    "    )\n",
    "    answer = bool(answer_t)\n",
    "    pred = bool(pred_t.round())\n",
    "    display(image)\n",
    "    print(question)\n",
    "    print(\"Ground truth: \", answer)\n",
    "    print(\"Prediction: \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
