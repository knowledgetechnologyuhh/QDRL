{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import Javascript\n",
    "from munch import Munch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "from tqdm.auto import trange\n",
    "\n",
    "from qsr_learning.main import (\n",
    "    draw,\n",
    "    emoji_names,\n",
    "    generate_negative_examples,\n",
    "    generate_positive_examples,\n",
    "    sample_objects,\n",
    ")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QSRData(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_images=4,\n",
    "        num_objects=2,\n",
    "        num_pos_questions_per_image=2,\n",
    "        num_neg_questions_per_image=2,\n",
    "    ):\n",
    "        self.images = []\n",
    "        self.questions = []\n",
    "        self.answers = []\n",
    "        for i in trange(num_images):\n",
    "            objects = sample_objects(num_objects=num_objects)\n",
    "            positive_examples = generate_positive_examples(\n",
    "                objects, num_pos_questions_per_image\n",
    "            )\n",
    "            negative_examples = generate_negative_examples(\n",
    "                objects, positive_examples, num_neg_questions_per_image\n",
    "            )\n",
    "            self.images.append(np.array(draw(objects))[:, :, :3])\n",
    "            for question in positive_examples:\n",
    "                self.questions.append({\"id\": i, \"question\": question})\n",
    "                self.answers.append({\"id\": i, \"answer\": True})\n",
    "            for question in negative_examples:\n",
    "                self.questions.append({\"id\": i, \"question\": question})\n",
    "                self.answers.append({\"id\": i, \"answer\": False})\n",
    "        super().__init__()\n",
    "        relation_names = {\"left_of\", \"right_of\", \"above\", \"below\"}\n",
    "        self.idx2word = dict(enumerate(sorted(set(emoji_names) | relation_names)))\n",
    "        self.word2idx = {self.idx2word[idx]: idx for idx in self.idx2word}\n",
    "        rgb_mean = 0.5\n",
    "        rgb_std = 0.5\n",
    "        self._transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize(rgb_mean, rgb_std)]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.questions[idx][\"id\"]\n",
    "        return (\n",
    "            self._transform(self.images[image_id]),\n",
    "            torch.tensor(\n",
    "                [self.word2idx[word] for word in self.questions[idx][\"question\"]]\n",
    "            ),\n",
    "            self.answers[idx][\"answer\"],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        # num_images * num_pos_questions_per_image * num_neg_questions_per_image\n",
    "        return len(self.questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_result(phases, result, loader):\n",
    "    result_dict = {\n",
    "        \"phase\": phase[:5],\n",
    "        \"loss\": result.total_loss[phase] / len(loader[phase].dataset),\n",
    "        \"accuracy\": result.num_correct[phase] / len(loader[phase].dataset),\n",
    "    }\n",
    "    return \"[{phase}] loss: {loss:4.3f}, acc: {accuracy:4.3f}\".format(**result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_encoder = resnet18(pretrained=True, progress=True)\n",
    "        self.embedding = nn.Embedding(config.num_embeddings, config.embedding_dim)\n",
    "        self.fc = nn.Linear(1000, 3 * config.embedding_dim)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, images, questions):\n",
    "        # image_features.shape = (batch_size, 3, 1000)\n",
    "        image_features = self.image_encoder(images)\n",
    "        # out.shape = (batch_size, self.embedding_dim)\n",
    "        out = self.fc(image_features)\n",
    "        # question_features.shape = (16, self.embedding_dim)\n",
    "        head_features = self.embedding(questions[:, 0])\n",
    "        relation_features = self.embedding(questions[:, 1])\n",
    "        tail_features = self.embedding(questions[:, 2])\n",
    "        question_features = torch.cat(\n",
    "            (head_features, relation_features, tail_features), dim=-1\n",
    "        )\n",
    "        out = (out * question_features).sum(-1)\n",
    "        return out.sigmoid()\n",
    "\n",
    "    def step(self, phase, batch, result):\n",
    "        self.train() if phase == \"train\" else self.eval()\n",
    "        torch.autograd.set_grad_enabled(phase == \"train\")\n",
    "        images, questions, answers = (item.to(device) for item in batch)\n",
    "        batch_size = images.shape[0]\n",
    "        self.zero_grad()\n",
    "        out = self(images, questions)\n",
    "        loss = criterion(out, answers.float()) / batch_size\n",
    "        result.total_loss[phase] += loss.item()\n",
    "        result.num_correct[phase] += ((out > 0.5) == answers).sum().item()\n",
    "        if phase == \"train\":\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Munch(\n",
    "    embedding_dim=20,\n",
    "    batch_size=16,\n",
    "    num_epochs=10,\n",
    "    data=Munch(\n",
    "        negative_sample_mixture=Munch(\n",
    "            head=0, tail=0, relation=1, head_relation=0, tail_relation=0, head_tail=0\n",
    "        ),\n",
    "        train=Munch(\n",
    "            num_images=128,\n",
    "            num_objects=2,\n",
    "            num_pos_questions_per_image=2,\n",
    "            num_neg_questions_per_image=2,\n",
    "        ),\n",
    "        validation=Munch(\n",
    "            num_images=16,\n",
    "            num_objects=2,\n",
    "            num_pos_questions_per_image=2,\n",
    "            num_neg_questions_per_image=2,\n",
    "        ),\n",
    "        test=Munch(\n",
    "            num_images=16,\n",
    "            num_objects=2,\n",
    "            num_pos_questions_per_image=2,\n",
    "            num_neg_questions_per_image=2,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "phases = [\"train\", \"validation\", \"test\"]\n",
    "data = Munch({phase: QSRData(**config.data[phase]) for phase in phases})\n",
    "config.num_embeddings = len(data.train.word2idx)\n",
    "loader = Munch(\n",
    "    {\n",
    "        phase: DataLoader(\n",
    "            data[phase], batch_size=config.batch_size, shuffle=True, num_workers=4\n",
    "        )\n",
    "        for phase in phases\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(config)\n",
    "net.to(device)\n",
    "criterion = nn.BCELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "phases = [\"train\", \"validation\"]\n",
    "with trange(config.num_epochs) as pbar:\n",
    "    for epoch in pbar:\n",
    "        result = Munch(\n",
    "            total_loss=Munch({phase: 0 for phase in phases}),\n",
    "            num_correct={phase: 0 for phase in phases},\n",
    "        )\n",
    "        for phase in phases:\n",
    "            for batch in loader[phase]:\n",
    "                net.step(phase, batch, result)\n",
    "            print(epoch, format_result(phases, result, loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = [\"test\"]\n",
    "result = Munch(\n",
    "    total_loss=Munch({phase: 0 for phase in phases}),\n",
    "    num_correct={phase: 0 for phase in phases},\n",
    ")\n",
    "\n",
    "for phase in phases:\n",
    "    for batch in loader[phase]:\n",
    "        net.step(phase, batch, result)\n",
    "print(format_result(phases, result, loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2image(x):\n",
    "    return Image.fromarray(\n",
    "        (255 * ((0.5 * x) + 0.5)).numpy().astype(\"uint8\").transpose(1, 2, 0)\n",
    "    )\n",
    "\n",
    "\n",
    "def tensor2question(x):\n",
    "    return [loader.train.dataset.idx2word[idx] for idx in x.numpy()]\n",
    "\n",
    "\n",
    "def tensor2answer(x):\n",
    "    return x.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader.test:\n",
    "    break\n",
    "images, questions, answers = (item.to(device) for item in batch)\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    print(net(images[:1], questions[:1]) > 0.5)\n",
    "    print(answers[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = []\n",
    "for item in loader[\"train\"].dataset.questions:\n",
    "    emojis.append(item[\"question\"][0])\n",
    "    emojis.append(item[\"question\"][2])\n",
    "emojis = set(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tensor2image(images[0].cpu()))\n",
    "display(tensor2question(questions[0].cpu()))\n",
    "display(tensor2answer(answers[0].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'kissing face with closed eyes' in emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Japanese discount button' in emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate negative examples with irrelevant objects satisfying the relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Location Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input: A scene containing one object\n",
    "- Output: The bounding box of the object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Javascript(\n",
    "    'var snd = new Audio(\"data:audio/wav;base64,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\"); snd.play(); new Notification(\"Cell Execution Has Finished\")'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "227"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
