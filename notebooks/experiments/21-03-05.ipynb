{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from ipywidgets import interact\n",
    "from qsr_learning.data import DRLDataset\n",
    "from qsr_learning.entity import emoji_names\n",
    "\n",
    "dataset = DRLDataset(\n",
    "    entity_names=[\"octopus\", \"trophy\"],\n",
    "    relation_names=[\"left_of\", \"right_of\"],\n",
    "    num_entities=4,\n",
    "    fixed_entities=None,\n",
    "    frame_of_reference=\"absolute\",\n",
    "    num_samples=1024,\n",
    "    w_range=(32, 32),\n",
    "    h_range=(32, 32),\n",
    "    theta_range=(0.0, 2 * math.pi),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "@interact(idx=(0, len(dataset) - 1))\n",
    "def display_sample(idx=0):\n",
    "    image_t, question_t, answer_t = dataset[idx]\n",
    "    image = Image.fromarray(\n",
    "        (255 * (dataset.std.view(-1, 1, 1) * image_t + dataset.mean.view(-1, 1, 1)))\n",
    "        .permute(1, 2, 0)\n",
    "        .numpy()\n",
    "        .astype(\"uint8\")\n",
    "    )\n",
    "    head, relation, tail = question_t.tolist()\n",
    "    question = (\n",
    "        dataset.idx2word[head],\n",
    "        dataset.idx2word[relation],\n",
    "        dataset.idx2word[tail],\n",
    "    )\n",
    "    answer = bool(answer_t)\n",
    "    display(image)\n",
    "    print(question)\n",
    "    print(\"Ground truth: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "class DRLNet(pl.LightningModule):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, vision_model: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Image encoder\n",
    "        resnet = getattr(torchvision.models, vision_model)(pretrained=True)\n",
    "        self.image_encoder = nn.Sequential(*deepcopy(list(resnet.children())[:-3]))\n",
    "        del resnet\n",
    "        # Freeze the image encoder weights\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Question encoder\n",
    "        self.question_encoder = nn.Identity()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "\n",
    "        # Fusion\n",
    "        self.fusion = nn.Identity()\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.fc = nn.Linear(\n",
    "            self.image_feature_size.numel(), self.question_feature_size.numel()\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def image_feature_size(self):\n",
    "        image = torch.rand((1, 3, 224, 224), device=self.device)\n",
    "        return self.image_encoder(image).shape\n",
    "\n",
    "    @property\n",
    "    def question_feature_size(self):\n",
    "        question = torch.ones((1, 3, self.embedding_dim), device=self.device)\n",
    "        return self.question_encoder(question).shape\n",
    "\n",
    "    def forward(self, images, questions):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.image_encoder.eval()\n",
    "\n",
    "        # Make prediction\n",
    "        images, questions, answers = batch\n",
    "        preds = self(images, questions)\n",
    "        loss = self.criterion(preds, answers)\n",
    "\n",
    "        # Logging\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_accuracy\", accuracy_score(answers, preds))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Make sure to filter the parameters based on `requires_grad`\n",
    "        return torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters))\n",
    "\n",
    "\n",
    "model = DRLNet(num_embeddings=10, embedding_dim=10, vision_model=\"resnet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.image_feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.question_feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer()\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
