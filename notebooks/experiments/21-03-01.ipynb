{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from munch import Munch\n",
    "from qsr_learning.data import DRLDataset\n",
    "from qsr_learning.entity import emoji_names\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DRLDataset(\n",
    "    entity_names=[\"octopus\", \"trophy\"],\n",
    "    relation_names=[\"left_of\", \"right_of\"],\n",
    "    num_entities=2,\n",
    "    fixed_entities=None,\n",
    "    frame_of_reference=\"absolute\",\n",
    "    num_samples=128,\n",
    "    w_range=(32, 32),\n",
    "    h_range=(32, 32),\n",
    "    theta_range=(0.0, 2 * math.pi),\n",
    "    num_questions_per_image=1,\n",
    "    random_seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRLNet(pl.LightningModule):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, vision_model: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Image encoder\n",
    "        resnet = getattr(torchvision.models, vision_model)(pretrained=True)\n",
    "        self.image_encoder = nn.Sequential(*deepcopy(list(resnet.children())[:-3]))\n",
    "        del resnet\n",
    "        # Freeze the image encoder weights\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Question encoder\n",
    "        self.question_encoder = nn.Identity()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "\n",
    "        # Fusion\n",
    "        self.fusion = nn.Identity()\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.fc = nn.Linear(\n",
    "            self.image_feature_size.numel(), self.question_feature_size.numel()\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def image_feature_size(self):\n",
    "        image = torch.rand((1, 3, 224, 224), device=self.device)\n",
    "        return self.image_encoder(image).shape\n",
    "\n",
    "    @property\n",
    "    def question_feature_size(self):\n",
    "        question = torch.ones((1, 3, self.embedding_dim), device=self.device)\n",
    "        return self.question_encoder(question).shape\n",
    "\n",
    "    def forward(self, images, questions):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.image_encoder.eval()\n",
    "\n",
    "        # Make prediction\n",
    "        images, questions, answers = batch\n",
    "        preds = self(images, questions)\n",
    "        loss = self.criterion(preds, answers)\n",
    "\n",
    "        # Logging\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_accuracy\", accuracy_score(answers, preds))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Make sure to filter the parameters based on `requires_grad`\n",
    "        return torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters))\n",
    "\n",
    "\n",
    "model = DRLNet(num_embeddings=10, embedding_dim=10, vision_model=\"resnet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.image_feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.question_feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer()\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
