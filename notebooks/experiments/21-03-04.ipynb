{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from munch import Munch\n",
    "from qsr_learning.data import DRLDataset\n",
    "from qsr_learning.entity import emoji_names\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "from qsr_learning.data import draw_entities\n",
    "from qsr_learning.entity import Entity\n",
    "from qsr_learning.relation import above, below, left_of, right_of\n",
    "\n",
    "\n",
    "def inside_canvas(entity: Entity, canvas_size: Tuple[int, int]) -> bool:\n",
    "    \"\"\"Check whether entity is in the canvas.\"\"\"\n",
    "    xs_inside_canvas = all(\n",
    "        (0 < entity.bbox[:, 0]) & (entity.bbox[:, 0] < canvas_size[0])\n",
    "    )\n",
    "    ys_inside_canvas = all(\n",
    "        (0 < entity.bbox[:, 1]) & (entity.bbox[:, 1] < canvas_size[1])\n",
    "    )\n",
    "    return xs_inside_canvas and ys_inside_canvas\n",
    "\n",
    "\n",
    "def generate_entities(\n",
    "    entity_names,\n",
    "    num_entities: int = 5,\n",
    "    frame_of_reference: str = \"absolute\",\n",
    "    w_range: Tuple[int, int] = (32, 32),\n",
    "    h_range: Tuple[int, int] = (32, 32),\n",
    "    theta_range: Tuple[float, float] = (0.0, 2 * math.pi),\n",
    "    canvas_size: Tuple[int, int] = (224, 224),\n",
    "    relations: List[Callable[[Entity, Entity], bool]] = [\n",
    "        left_of,\n",
    "        right_of,\n",
    "        above,\n",
    "        below,\n",
    "    ],\n",
    ") -> List[Entity]:\n",
    "    \"\"\"\n",
    "    :param canvas_size: (width, height)\n",
    "    \"\"\"\n",
    "    entity_names_copy = deepcopy(entity_names)\n",
    "    random.shuffle(entity_names_copy)\n",
    "\n",
    "    entities_in_canvas = False\n",
    "    while not entities_in_canvas:\n",
    "        entities = []\n",
    "        for name in entity_names_copy[:num_entities]:\n",
    "            # Rotate and translate the entities.            \n",
    "            theta = random.uniform(*theta_range)\n",
    "            p = (random.uniform(0, canvas_size[0]), random.uniform(0, canvas_size[1]))\n",
    "            entity = Entity(\n",
    "                name=name,\n",
    "                frame_of_reference=frame_of_reference,\n",
    "                p=p,\n",
    "                theta=theta,\n",
    "                size=(random.randint(*w_range), (random.randint(*h_range))),\n",
    "            )\n",
    "            entities.append(entity)\n",
    "        # Ensure that all entities are inside the canvas\n",
    "        entities_in_canvas = all(\n",
    "            inside_canvas(entity, canvas_size) for entity in entities\n",
    "        )\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "\n",
    "from qsr_learning.relation import above, below, left_of, right_of\n",
    "\n",
    "Question = namedtuple(\"Question\", [\"head\", \"relation\", \"tail\"])\n",
    "self = Munch()\n",
    "self.entity_names = [\"octopus\", \"trophy\"]\n",
    "self.num_entities = 2\n",
    "self.frame_of_reference = \"absolute\"\n",
    "self.w_range = (32, 32)\n",
    "self.h_range = (32, 32)\n",
    "self.add_bbox = False\n",
    "self.theta_range = (0, 2 * math.pi)\n",
    "self.canvas_size = (224, 224)\n",
    "self.relations = [above, below, left_of, right_of]\n",
    "\n",
    "\n",
    "def gen_sample():\n",
    "    satisfied = []\n",
    "    while not satisfied:\n",
    "        # TODO: continue here\n",
    "        entities = generate_entities(\n",
    "            self.entity_names,\n",
    "            self.num_entities,\n",
    "            self.frame_of_reference,\n",
    "            w_range=self.w_range,\n",
    "            h_range=self.h_range,\n",
    "            theta_range=self.theta_range,\n",
    "            canvas_size=self.canvas_size,\n",
    "            relations=self.relations,\n",
    "        )\n",
    "        head, tail = random.sample(entities, 2)\n",
    "        answer = random.randint(0, 1)\n",
    "        indices = list(range(len(self.relations)))\n",
    "        random.shuffle(indices)\n",
    "        satisfied = [\n",
    "            self.relations[i] for i in indices if self.relations[i](head, tail)\n",
    "        ]\n",
    "    dissatisified = list(set(self.relations) - set(satisfied))\n",
    "    relation = random.choice(satisfied) if answer else random.choice(dissatisified)\n",
    "    image = draw_entities(\n",
    "        entities, canvas_size=self.canvas_size, show_bbox=self.add_bbox\n",
    "    )\n",
    "    question = Question(head.name, relation.__name__, tail.name)\n",
    "    return image, question, answer\n",
    "\n",
    "\n",
    "image, question, answer = gen_sample()\n",
    "display(image, question, bool(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "Question = namedtuple(\"Question\", [\"head\", \"relation\", \"tail\"])\n",
    "\n",
    "\n",
    "class DRLDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        entity_names,\n",
    "        relation_names,\n",
    "        num_entities,\n",
    "        fixed_entities,\n",
    "        frame_of_reference,\n",
    "        w_range,\n",
    "        h_range,\n",
    "        theta_range,\n",
    "        num_samples,\n",
    "        filter_fn=None,\n",
    "        show_bbox=False,\n",
    "        orientation_marker=False,\n",
    "        transform=None,\n",
    "        num_questions_per_image=1,\n",
    "        random_seed=0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param num_questions_per_image: the (maximal) number of questions generated for each image.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.entity_names = entity_names\n",
    "        self.relations = [\n",
    "            getattr(qsr_learning.relation, relation_name)\n",
    "            for relation_name in relation_names\n",
    "        ]\n",
    "        self.num_entities = num_entities\n",
    "        self.fixed_entities = fixed_entities  # predefined entites\n",
    "        self.w_range = (32, 32)\n",
    "        self.h_range = (32, 32)\n",
    "        self.frame_of_reference = frame_of_reference\n",
    "        self.theta_range = (0, 2 * math.pi)\n",
    "        self.num_samples = num_samples\n",
    "        self.filter_fn = filter_fn\n",
    "        self.show_bbox = show_bbox\n",
    "        self.orientation_marker = orientation_marker\n",
    "\n",
    "        if not transform:\n",
    "            self.mean, self.std = get_mean_and_std(\n",
    "                entity_names,\n",
    "                relation_names,\n",
    "                num_entities,\n",
    "                w_range,\n",
    "                h_range,\n",
    "            )\n",
    "            self.transform = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(self.mean, self.std),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        self.num_questions_per_image = num_questions_per_image\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        torch.manual_seed(random_seed)\n",
    "\n",
    "        self.idx2ent, self.ent2idx = {}, {}\n",
    "        for idx, entity_name in enumerate(sorted(entity_names)):\n",
    "            self.idx2ent[idx] = entity_name\n",
    "            self.ent2idx[entity_name] = idx\n",
    "\n",
    "        self.idx2rel, self.rel2idx = {}, {}\n",
    "        for idx, relation_name in enumerate(sorted(relation_names)):\n",
    "            self.idx2rel[idx] = relation_name\n",
    "            self.rel2idx[relation_name] = idx\n",
    "\n",
    "        self.image: Dict[int, PIL.Image] = {}\n",
    "        self.questions: Dict[int, Questions] = {}\n",
    "        self.answers: Dict[int, Answers] = {}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.generate_sample()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def gen_sample(self):\n",
    "        while True:\n",
    "            entities = generate_entities(\n",
    "                entity_names=self.entity_names,\n",
    "                num_entities=self.num_entities,\n",
    "                frame_of_reference=self.frame_of_reference,\n",
    "                w_range=self.w_range,\n",
    "                h_range=self.h_range,\n",
    "                theta_range=self.theta_range,\n",
    "            )\n",
    "            head, tail = random.sample(entities, 2)\n",
    "            relations_shuffled = deepcopy(self.relations)\n",
    "            random.shuffle(relations_shuffled)\n",
    "            answer = random.randint(0, 1)\n",
    "            for relation in relations_shuffled:\n",
    "                if relation(head, tail) == answer:\n",
    "                    break\n",
    "\n",
    "            image = draw_entities(\n",
    "                entities,\n",
    "                show_bbox=self.show_bbox,\n",
    "                orientation_marker=self.orientation_marker,\n",
    "            )\n",
    "            background = Image.new(\"RGBA\", image.size, (0, 0, 0))\n",
    "            image = Image.alpha_composite(background, image).convert(\"RGB\")\n",
    "            question = Question(head.name, relation.__name__, tail.name)\n",
    "\n",
    "            return image, question, answer\n",
    "\n",
    "    def gen_sample(self):\n",
    "        entities = generate_entities(\n",
    "            entity_names=self.entity_names,\n",
    "            num_entities=self.num_entities,\n",
    "            frame_of_reference=self.frame_of_reference,\n",
    "            w_range=self.w_range,\n",
    "            h_range=self.h_range,\n",
    "            theta_range=self.theta_range,\n",
    "        )\n",
    "\n",
    "        image = draw_entities(\n",
    "            entities,\n",
    "            show_bbox=self.show_bbox,\n",
    "            orientation_marker=self.orientation_marker,\n",
    "        )\n",
    "        background = Image.new(\"RGBA\", image.size, (0, 0, 0))\n",
    "        image = Image.alpha_composite(background, image).convert(\"RGB\")\n",
    "\n",
    "        question, answer = gen_qa(entities, self.relations)\n",
    "\n",
    "        def q2t(question: Question):\n",
    "            \"\"\"Convert question to tensor\"\"\"\n",
    "            return torch.tensor([self.word2idx[w] for w in question], dtype=torch.int64)\n",
    "\n",
    "        return self.transform(image), q2t(question), torch.tensor(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DRLDataset(\n",
    "    entity_names=[\"octopus\", \"trophy\"],\n",
    "    relation_names=[\"left_of\", \"right_of\"],\n",
    "    num_entities=2,\n",
    "    fixed_entities=None,\n",
    "    frame_of_reference=\"absolute\",\n",
    "    num_samples=128,\n",
    "    w_range=(32, 32),\n",
    "    h_range=(32, 32),\n",
    "    theta_range=(0.0, 2 * math.pi),\n",
    "    num_questions_per_image=1,\n",
    "    random_seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRLNet(pl.LightningModule):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, vision_model: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Image encoder\n",
    "        resnet = getattr(torchvision.models, vision_model)(pretrained=True)\n",
    "        self.image_encoder = nn.Sequential(*deepcopy(list(resnet.children())[:-3]))\n",
    "        del resnet\n",
    "        # Freeze the image encoder weights\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Question encoder\n",
    "        self.question_encoder = nn.Identity()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "\n",
    "        # Fusion\n",
    "        self.fusion = nn.Identity()\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.fc = nn.Linear(\n",
    "            self.image_feature_size.numel(), self.question_feature_size.numel()\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def image_feature_size(self):\n",
    "        image = torch.rand((1, 3, 224, 224), device=self.device)\n",
    "        return self.image_encoder(image).shape\n",
    "\n",
    "    @property\n",
    "    def question_feature_size(self):\n",
    "        question = torch.ones((1, 3, self.embedding_dim), device=self.device)\n",
    "        return self.question_encoder(question).shape\n",
    "\n",
    "    def forward(self, images, questions):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.image_encoder.eval()\n",
    "\n",
    "        # Make prediction\n",
    "        images, questions, answers = batch\n",
    "        preds = self(images, questions)\n",
    "        loss = self.criterion(preds, answers)\n",
    "\n",
    "        # Logging\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_accuracy\", accuracy_score(answers, preds))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Make sure to filter the parameters based on `requires_grad`\n",
    "        return torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters))\n",
    "\n",
    "\n",
    "model = DRLNet(num_embeddings=10, embedding_dim=10, vision_model=\"resnet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.image_feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.question_feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer()\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
